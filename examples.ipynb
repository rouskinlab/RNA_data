{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rna_data import CreateDataset\n",
    "\n",
    "path_test_data = 'data/demo_datasets'\n",
    "path_test_data_sequences = 'data/sequences.fasta'\n",
    "path_test_data_dreem = 'data/dreem_output.json'\n",
    "path_test_data_ct = 'data/ct_files'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('rm -rf data/demo_datasets')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset from local files\n",
    "\n",
    "## From fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreateDatasetFromFasta @data/demo_datasets/sequences\n",
      "main folder: data/demo_datasets/sequences\n",
      "npy files: ['data/demo_datasets/sequences/npy/placeholder.npy']\n",
      "json file: data/demo_datasets/sequences/data.json\n",
      "source files: ['data/demo_datasets/sequences/source/sequences.fasta']\n",
      "info file: data/demo_datasets/sequences/info.json\n"
     ]
    }
   ],
   "source": [
    "dataset = CreateDataset.from_fasta(\n",
    "    name= 'sequences', # name of the dataset or something else\n",
    "    path_in = path_test_data_sequences, \n",
    "    path_out = path_test_data, \n",
    "    generate_npy=True, \n",
    "    predict_structure=True,\n",
    "    predict_dms=True)\n",
    "\n",
    "print(dataset)\n",
    "print('main folder:',dataset.get_main_folder())\n",
    "print('npy files:',dataset.get_npy_files())\n",
    "print('json file:',dataset.get_json())\n",
    "print('source files:',dataset.get_source_files())\n",
    "print('info file:',dataset.get_info_file())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From DREEM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreateDatasetFromDreemOutput @data/demo_datasets/from_dreem_output\n",
      "main folder: data/demo_datasets/from_dreem_output\n",
      "npy files: ['data/demo_datasets/from_dreem_output/npy/placeholder.npy']\n",
      "json file: data/demo_datasets/from_dreem_output/data.json\n",
      "source files: ['data/demo_datasets/from_dreem_output/source/dreem_output.json']\n",
      "info file: data/demo_datasets/from_dreem_output/info.json\n"
     ]
    }
   ],
   "source": [
    "dataset = CreateDataset.from_dreem_output(\n",
    "    name='from_dreem_output', # name of the dataset or something else\n",
    "    path_in=path_test_data_dreem, \n",
    "    path_out=path_test_data, \n",
    "    generate_npy=True, \n",
    "    predict_structure=True)\n",
    "\n",
    "print(dataset)\n",
    "print('main folder:',dataset.get_main_folder())\n",
    "print('npy files:',dataset.get_npy_files())\n",
    "print('json file:',dataset.get_json())\n",
    "print('source files:',dataset.get_source_files())\n",
    "print('info file:',dataset.get_info_file())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a list of CT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreateDatasetFromCTfolder @data/demo_datasets/from_ct_folder\n",
      "main folder: data/demo_datasets/from_ct_folder\n",
      "npy files: ['data/demo_datasets/from_ct_folder/npy/placeholder.npy']\n",
      "json file: data/demo_datasets/from_ct_folder/data.json\n",
      "source files: ['data/demo_datasets/from_ct_folder/source/ct_files']\n",
      "info file: data/demo_datasets/from_ct_folder/info.json\n"
     ]
    }
   ],
   "source": [
    "dataset = CreateDataset.from_ct_folder(\n",
    "    name='from_ct_folder', # name of the dataset or something else\n",
    "    path_in=path_test_data_ct, \n",
    "    path_out=path_test_data, \n",
    "    generate_npy=True, \n",
    "    predict_dms=False)  # won't take the structure from the ct files into account\n",
    "\n",
    "print(dataset)\n",
    "print('main folder:',dataset.get_main_folder())\n",
    "print('npy files:',dataset.get_npy_files())\n",
    "print('json file:',dataset.get_json())\n",
    "print('source files:',dataset.get_source_files())\n",
    "print('info file:',dataset.get_info_file())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push a dataset to the server\n",
    "\n",
    "## Login (if not already done)\n",
    "\n",
    "### Join the Hugging Face Community\n",
    "\n",
    "[Hugging face](https://huggingface.co)\n",
    "\n",
    "### Join the Rouskinlab organization\n",
    "\n",
    "[Rouskin Lab on Hugging Face](https://huggingface.co/RouskinLab)\n",
    "\n",
    "### Login to the Hugging Face Hub\n",
    "\n",
    "1. Get a token from https://huggingface.co/settings/token\n",
    "\n",
    "2. Run in a terminal this command line and follow the instructions:\n",
    "\n",
    "`huggingface-cli login`\n",
    "\n",
    "## Push the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example using a fasta file\n",
    "dataset = CreateDataset.from_fasta(\n",
    "    name= 'sequences', # name of the dataset or something else.\n",
    "    path_in = path_test_data_sequences, \n",
    "    path_out = path_test_data, \n",
    "    generate_npy=True, \n",
    "    predict_structure=True,\n",
    "    predict_dms=True)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "placeholder.npy: 100%|██████████| 4.00/4.00 [00:01<00:00, 2.75B/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/rouskinlab/test2/tree/main/'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi(token=\"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\")\n",
    "#api.create_repo(\n",
    "#    repo_id=\"rouskinlab/test2\",\n",
    "#    private=False,\n",
    "#)\n",
    "api.upload_folder(\n",
    "    folder_path=dataset.get_main_folder(),\n",
    "    path_in_repo=\"\",\n",
    "    repo_id=\"rouskinlab/test2\",\n",
    "   # repo_type=\"space\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        #'data': Dataset.from_json(dataset.get_json()),\n",
    "        'structure': Dataset.from_dict({'structure': ['TODO']}),\n",
    "       # 'DMS': Dataset.from_dict({'DMS': ['TODO']}),\n",
    "       # 'info': Dataset.from_json(dataset.get_info_file()),\n",
    "      #  'source': Dataset.from_dict({'source': ['TODO']})\n",
    "     }#\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 3560.53ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      "Downloading metadata: 100%|██████████| 344/344 [00:00<00:00, 1.90MB/s]\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "Dataset.from_dict({'structure': ['TODO']}).push_to_hub(\"rouskinlab/test_dataset\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.push_to_hub(overwrite=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
